{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Applied ML Projects","text":"<p>Author: Aaron</p> <p>Projects index:</p> <ul> <li>Final Project: Predicting Medical Cost with Regression</li> <li>Midterm Project: Banknote Authentication Analysis</li> <li>Project 02: Exploring the Titanic Dataset</li> <li>Project 03: Three Model Types with the Titanic Dataset</li> <li>Project 04: Predicting a Continuous Target with Regression (Titanic)</li> <li>Project 05: Research on Ensemble Models (Red Wine)</li> </ul>"},{"location":"final/","title":"Final - Predicting Medical Cost with Regression","text":"<p>Author: AARON  Date: November 24, 2025  Objective: Create a Regression Model to predict Medical Claims.</p>"},{"location":"final/#introduction","title":"Introduction","text":"<ul> <li>Using the Medical Cost Personal Dataset, decide what inputs to use in Regression model to predict Medical Claims.</li> <li>Try alternate methods or engineered variables to find an optimal model.</li> </ul>"},{"location":"final/#section-1-import-and-inspect-the-data","title":"Section 1. Import and Inspect the Data","text":""},{"location":"final/#reflection-1","title":"Reflection 1:","text":"<p>What do you notice about the dataset? The dataset is fully populated with 1338 records. Are there any data issues?  There were no missing values.</p>"},{"location":"final/#section-2-data-exploration-and-preparation","title":"Section 2. Data Exploration and Preparation","text":""},{"location":"final/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":""},{"location":"final/#reflection-2-and-3","title":"Reflection 2 and 3:","text":"<ul> <li>This dataset just had a handful of inputs to choose from.  I elected to add age and bmi then multiply that sum by 3 if a smoker as case 2.</li> <li>I also decided to include all inputs in case 1 to compare the results to my single engineered input.</li> </ul>"},{"location":"final/#section-4-train-a-regression-model-linear-regression","title":"Section 4. Train a Regression Model (Linear Regression)","text":""},{"location":"final/#reflection-4","title":"Reflection 4:","text":"<p>Compare the train vs test results for each.</p> Model Type Case Features Used Training R\u00b2 Test R\u00b2 RMSE MAE Notes Linear Regression Case 1 all features 0.7376 0.8001 30552437 4013 - Linear Regression Case 2 3ptindex 0.7454 0.8247 26792600 3659 - <p></p> <ul> <li>Did Case 1 overfit or underfit? Case 1 is in the nominal range.</li> <li>Did Case 2 overfit or underfit? Case 2 is in the nominal range.</li> </ul>"},{"location":"final/#adding-age-bmi-2-smoker-1","title":"Adding Age + BMI * (2 * Smoker + 1)","text":"<ul> <li>Did adding 3pt index improve the model: The model improved measurably from Case 1 to Case 2.</li> <li>Propose a possible explanation (consider how age might affect ticket price, and whether the data supports that):  The 3ptindex formula keeps non-smoker claims as is in determining claims.  But if someone is a smoker it amplifies the age and bmi by 3 times.  This helps the linear regression with these higher values.  </li> <li>At first I just had 2 times, but I tried 3 times and the results were better.  </li> <li>I did not need to use the other parameters either and still achieved better results.</li> <li>I wanted just a single variable to see the plot and in some ways stumbled into a great model.</li> </ul>"},{"location":"final/#section-5-compare-alternative-models","title":"Section 5. Compare Alternative Models","text":""},{"location":"final/#55-reflection","title":"5.5 Reflection","text":"<ul> <li>What patterns does the cubic model seem to capture:  It is very hard to know for case 1 with all the inputs.  For case 2 it matched the data slightly better than linear regression.</li> <li>Where does it perform well or poorly:  According to the residuals plot the result is very balanced for case 1.  I see some values on both sides of zero which is desirable. For case 2, there is still some balance, but it is not quite as good as case 1.</li> <li>Did the polynomial fit outperform linear regression: Yes the R\u00b2 value 0.888 for poly but 0.80 for Linear for case 1.  For case 2 the results were almost the same.</li> <li>Where (on the graph or among which kinds of data points) does it fit best:  The prediction works well for smokers and non-smokers for case 1 and case 2. </li> </ul> Model Type Case Features Used Test R\u00b2 RMSE MAE Notes Linear Case 1 All Inputs 0.800 30552437 4013 - Poly Cubic Case 1 All Inputs 0.888 17093508 2772 - Linear Case 2 3ptindex 0.825 26792600 3659 - Poly Cubic Case 2 3ptindex 0.824 26873381 3686 -"},{"location":"final/#56-compare-all-models","title":"5.6 Compare All Models","text":"Model Type Case Features Used Test R\u00b2 RMSE MAE Notes Linear Case 1 All Imputs 0.800 30552437 4013 - Ridge Case 1 All Imputs 0.800 30632410 4025 - ElasticNet Case 1 All Imputs 0.610 59577764 5621 - Poly Cubic Case 1 All Imputs 0.888 17093508 2772 - Linear Case 2 3ptindex 0.825 26792600 3659 - Poly Cubic Case 2 3ptindex 0.824 26873381 3686 -"},{"location":"final/#reflection-57","title":"Reflection 5.7:","text":"<ul> <li>There was just one model that outperformed and that was Polynomial Cubic on Case 1.  It did much better than all other models.  I'm not sure why as it is hard to visualize the data.</li> <li>The other models did not perform better than Linear Regression on Case 1.  </li> <li>The Polynomial Cubic on Case 2 was slightly underperforming the Linear Regression on Case 2.</li> </ul>"},{"location":"final/#section-6-final-thoughts-insights","title":"Section 6. Final Thoughts &amp; Insights","text":""},{"location":"final/#61-summarize-findings","title":"6.1 Summarize Findings","text":"<ul> <li> <p>What features were most useful?  The engineered feature 3ptindex was a very powerful predictor of claims.</p> </li> <li> <p>What regression model performed best?  The Polynomial Cubic Model iterated on Case 1 had the best metrics.  The next best model was Case 2 and a simple Linear Regression.  It performed better than all of Case 1 except the Polynomial Cubic Model, and case 1 had all the inputs whereas Case 2 just had the single 3ptindex input.  </p> </li> <li>You could see this in the residual graphics.  The Case 1 Cubic Model did have a quality distribution in the residual graphic.  To the eye the Case 2 Linear Model had the next best residual graphic.  Meaning the data points were around the zero line and not all on one side.  One also wants to see the points close to 0.</li> </ul>"},{"location":"final/#62-discuss-challenges-faced","title":"6.2 Discuss challenges faced.","text":"<ul> <li>I had a hard time visualizing all the inputs for case 1.  That is what drove me to find one variable so I could see the regression line and polynomial cubic line.</li> <li>It was a challenge finding a single engineered variable that described the data.  But I did, and I am very pleased with the results.  It performed better that most of the models with all the inputs.</li> </ul>"},{"location":"final/#63-if-you-had-more-time-what-would-you-try-next","title":"6.3 If you had more time, what would you try next?","text":"<ul> <li>I would try adding more inputs to Case 2. </li> <li>I would try Ridge Regression and ElasticNet, but try varying the parameters.  </li> <li>I also would try voting.</li> </ul>"},{"location":"final/#code","title":"Code","text":"<ul> <li>The notebook for this project can be found at: regression_aaron.ipynb</li> </ul>"},{"location":"final/#peer-review","title":"Peer Review","text":"<ul> <li>I reviewed Womenker Karto's Final Project.  The review can be found at: Regression Analysis Project</li> </ul>"},{"location":"final/#how-to-run-the-jupyter-notebook","title":"How to Run the Jupyter Notebook","text":"<p>This Jupyter Notebook utilizes some packages in the uv environment and will install some dependencies.  The following set of commands would help you complete these two objectives. </p> <pre><code>uv venv\nuv python pin 3.12\nuv sync --extra dev --extra docs --upgrade\nuv run python --version\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <p>macOS / Linux / WSL:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>There are imports at the top of the Jupyter Notebook that should also be run to utilize the installed software and run the code in the rest of the Jupyter Notebook.</p>"},{"location":"final/peer_review/","title":"Machine Learning Final Project: Peer Review","text":""},{"location":"final/peer_review/#project-reviewed-regression-analysis-project","title":"Project Reviewed: Regression Analysis Project","text":"<p>Author: Womenker Karto Date: November 24, 2025  Objective: Analyze the work done in this project and give feedback within the four sections below.</p>"},{"location":"final/peer_review/#notebook-link","title":"Notebook Link:","text":"<p>Regression Analysis Project</p>"},{"location":"final/peer_review/#1-clarity-organization-is-the-notebook-structured-and-easy-to-follow","title":"1 Clarity &amp; Organization (Is the notebook structured and easy to follow?)","text":"<p>I appreciated the project flow and additional subsections.  Quality detail was provided.  It was helpful to understand the project and some of the building blocks of regression.</p>"},{"location":"final/peer_review/#2-feature-selection-justification-do-the-chosen-features-make-sense-given-the-objectives","title":"2 Feature Selection &amp; Justification (Do the chosen features make sense given the objectives?)","text":"<p>I may not fully understand the engineered feature bmi_smoker.  It looks like if smoker than multiply 1 * BMI which outputs BMI.  It looks like if non-smoker then multiply 0 * BMI which outputs 0. It may actually work though because you are just getting &gt;0 values when a smoker.  So the models may pick up on that.</p> <p>I am not sure what bmi_age_interaction, bmi_squared features were in Reflection 4.  They were not mentioned in Section 3: Feature Selection.</p>"},{"location":"final/peer_review/#3-model-performance-comparisons-are-the-results-and-comparisons-clearly-explained","title":"3 Model Performance &amp; Comparisons (Are the results and comparisons clearly explained?)","text":"<p>The R2 value of 0.8668 is very good.  Karto's mix of inputs and engineered features worked well. He explains the results well. I like how he examined the more complicated model pipelines.  Even though the results were not as good, it was good to see this data.</p>"},{"location":"final/peer_review/#4-reflection-quality-are-insights-well-thought-out","title":"4 Reflection Quality (Are insights well thought out?)","text":"<p>I found the reflections very informative and helpful.  He draws some good conclusions and helps picture what this dataset has to offer in addition to making a quality prediction.</p>"},{"location":"midterm/","title":"Machine Learning Midterm Project: Banknote Authentication Analysis","text":"<p>Author: AARON  Date: November 10, 2025  Objective: Setup two model types, Neural Network and Decision Tree, and use these to predict authenticity of bank notes from the UCI Banknote Authentication Dataset.</p>"},{"location":"midterm/#introduction","title":"Introduction","text":"<ul> <li>1st, This project uses the UCI Banknote Authentication Dataset to Explore and Clean data, choose features used to predict authenticity of bank notes, and split the dataset into train and test subsets.</li> <li>2nd, Two model types, Neural Network and Decision Tree, will be used to predict authenticity of bank notes from the UCI Banknote Authentication Dataset.</li> <li>3rd, Reflections are include from the sections of the Jupyter Notebook.  These reflections are summaries of why choices were made and key points from the data analysis.  The link to the Jupyter Notebook is: Midterm Notebook </li> </ul>"},{"location":"midterm/#reflection-1","title":"Reflection 1:","text":"<ul> <li>How many data instances are there? 1372</li> <li>How many features are there? 4</li> <li>What are the names?  Variance;     Skewness;     Curtosis;      Entropy;            </li> <li>Are there any missing values? No</li> <li>Are there any non-numeric features? Yes.  All four features are numeric.</li> <li>Are the data instances sorted on any of the attributes?  There is no a sort on any attribute.</li> <li>What two different features have the highest correlation? Skewness and Entropy against Variance</li> <li>Are there any categorical features that might be useful for prediction?  Perhaps entropy and curtosis would be a good place to start.</li> </ul>"},{"location":"midterm/#reflection-2-and-3","title":"Reflection 2 and 3:","text":"<ul> <li>Why are these features selected?  Curtosis, Variance, and Skewness were selected based on reviewing the overview of the data in the scatter matrix.  I saw that when skewness and variance are high in value there is a tendency for genuine notes.  I also saw when there is a lower curtosis value the tendency it towards genuine notes.  So I want to focus on these.  </li> <li>I chose for case 3 a feature engineered variable where I add the values of skewness and variance to heighten the positive values weight of the parameters.  I think that will help the model out even more.</li> </ul> <p>Decision Tree Data</p> Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree Case 1 Curtosis, Variance 91.6% 91.7% 91.6% 91.6% - Case 2 Skewness, Variance 93.1% 93.2% 93.1% 93.1% - Case 3 Curtosis, skew_var 98.6% 98.6% 98.6% 98.5% -"},{"location":"midterm/#reflection-4-decision-tree-models","title":"Reflection 4 (Decision Tree Models):","text":"<ul> <li>How well did the different cases perform?  I was happy to see all of the parameters at or above 91%.</li> <li>Are there any surprising results?  I was surprised that all the training values were 100%.  I'm not sure why that is. I was surprised to see that the engineered feature had such a big impact.</li> <li>Which inputs worked better? I think Skewness along with Variance is a key parameter since Case 2 outperformed.  I am excited to see how these cases will turn out in a Neural Network model.</li> </ul>"},{"location":"midterm/#reflection-5-neural-network-models","title":"Reflection 5 (Neural Network Models):","text":"<ul> <li>How well did each of these neural network cases perform?  The Neural network cases with Skewness outperformed the decision tree.  But Case 1 for the NN was significantly worse at predicting that all other instances in NN or Decision Tree</li> <li>Are there any surprising results or insights?  I am surprised Skewness seemed to have such a strong effect on the metrics.  The only difference between Case 1 and Case 3 was skewness and it had a 7% difference in the results.</li> <li>Why might one model outperform the others?  Looking at the Neural Network graphs it looks like a surface with points plotted over areas that the model predicts.  That makes sense to me that the NN can make good predictions over asking questions in the decision tree.</li> </ul> <p>Neural Network and Decision Tree Data Metric</p> Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree Case 1 Curtosis, Variance 91.6% 91.7% 91.6% 91.6% - Case 2 Skewness, Variance 93.1% 93.2% 93.1% 93.1% - Case 3 Curtosis, skew_var 98.6% 98.6% 98.6% 98.5% - --------------- -------- -------------------- --------- ----------- -------- ----------- ------- Neural Network Case 1 Curtosis, Variance 86.2% 86.2% 86.2% 86.2% - Case 2 Skewness, Variance 94.6% 94.6% 94.6% 94.6% - Case 3 Curtosis, skew_var 99.6% 99.6% 99.6% 99.6% -"},{"location":"midterm/#section-6-final-thoughts-insights","title":"Section 6. Final Thoughts &amp; Insights","text":"<ul> <li>The accuracy and other metrics were higher for this dataset compared to the Titanic dataset.  It was nice to see a higher percentage for these metrics.  </li> <li>I'm not sure why the Neural Network for Case 1 shows such a low percentage compared to other NN Cases, but also compared to Case 1 in the Decision tree.  It seems there must be an error, but I have checked a few times and cannot locate why this happened.</li> <li>I got the idea to combine the Skewness and Variance from the Scatter Matrix.  I saw Genuine notes had higher values for both these parameters.  So I thought I could combine them into one variable and that worked out great.</li> <li>I have not tried three parameters in a neural network.  I liked the decisions surface for two inputs.  So I combined Skewness and Variance.</li> <li>At first I used the Case 3 engineered parameter as Skewness Times Variance.  It performed a little worse than CASE 2.  I thought I would try adding them instead, towards the end of the project, and the numbers returned tremendous.  There was only one misclassification in the CASE 3 Neural Network results.</li> <li> </li> <li> <p>I was happy with these results and glad inspection of the scatter plot let me to combine the correct features.</p> </li> </ul>"},{"location":"midterm/#peer-review","title":"Peer Review","text":"<ul> <li>I reviewed Alissa Beaderstadt's Midterm Project.  The review can be found at: Beaderstadt UCI Mushroom Dataset</li> </ul>"},{"location":"midterm/#how-to-run-the-jupyter-notebook","title":"How to Run the Jupyter Notebook","text":"<p>This Jupyter Notebook utilizes some packages in the uv environment and will install some dependencies.  The following set of commands would help you complete these two objectives. </p> <pre><code>uv venv\nuv python pin 3.12\nuv sync --extra dev --extra docs --upgrade\nuv run python --version\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <p>macOS / Linux / WSL:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>There are imports at the top of the Jupyter Notebook that should also be run to utilize the installed software and run the code in the rest of the Jupyter Notebook.</p>"},{"location":"midterm/peer_review/","title":"Machine Learning Midterm Project: Peer Review","text":""},{"location":"midterm/peer_review/#project-reviewed-beaderstadt-uci-mushroom-dataset-classification-machine-learning-midterm","title":"Project Reviewed: Beaderstadt UCI Mushroom Dataset: Classification Machine Learning Midterm","text":"<p>Author: Alissa Beaderstadt Date: November 6, 2025  Objective: Analyze the work done in this project and give feedback within the four sections below.</p>"},{"location":"midterm/peer_review/#notebook-link","title":"Notebook Link:","text":"<p>https://github.com/abeaderstadt/ml_classification_beaderstadt/blob/main/classification_beaderstadt.ipynb</p>"},{"location":"midterm/peer_review/#1-clarity-organization-is-the-notebook-structured-and-easy-to-follow","title":"1 Clarity &amp; Organization (Is the notebook structured and easy to follow?)","text":"<p>I appreciated the project flow and additional subsections.  For example Section 6 was well laid out and summarized the findings well.</p>"},{"location":"midterm/peer_review/#2-feature-selection-justification-do-the-chosen-features-make-sense-given-the-objectives","title":"2 Feature Selection &amp; Justification (Do the chosen features make sense given the objectives?)","text":"<p>There were a lot of features to choose from.  I think the graphs were important to help decide which features to use in the model.  I think the selections were made thoughtfully and that decision worked all well as seen in the results.</p>"},{"location":"midterm/peer_review/#3-model-performance-comparisons-are-the-results-and-comparisons-clearly-explained","title":"3 Model Performance &amp; Comparisons (Are the results and comparisons clearly explained?)","text":"<p>Probably the oddest thing I see was the results were the same both for the Decision Tree and Random Forest models.  That seems strange to me, but makes me think that the nine poisonous misclassified as edible really had characteristics like the edible mushrooms in the features selected for the model. </p> <p>I think if any more improvement were needed more features would need to be introduced to the model.  I would have liked to hear Alissa's comments on this point.</p>"},{"location":"midterm/peer_review/#4-reflection-quality-are-insights-well-thought-out","title":"4 Reflection Quality (Are insights well thought out?)","text":"<p>I found the reflections very informative and helpful.  I had never seen this dataset before, and I feel I have a good understanding of it now after reading through Alissa's midterm project.</p>"},{"location":"project01/","title":"Project 01","text":""},{"location":"project01/#overview","title":"Overview","text":"<p>Businesses and organizations often need to understand the relationships between different factors to make better decisions. For example, a company may want to predict the fuel efficiency of a car based on its weight and engine size or estimate home prices based on square footage and location. Regression analysis helps identify and quantify these relationships between numerical features, providing insights that can be used for forecasting and decision-making.</p> <p>This project demonstrates your ability to apply regression modeling techniques to a real-world dataset. You will: - Load and explore a dataset. - Choose and justify features for predicting a target variable. - Train a regression model and evaluate performance. - Compare multiple regression approaches. - Document your work in a structured Jupyter Notebook.</p>"},{"location":"project01/#dataset","title":"Dataset","text":"<p>Housing Prices Dataset (Predict home values based on features like square footage and location) - We use the built-in dataset from scikit-learn:    - <code>from sklearn.datasets import fetch_california_housing</code> - Additional dataset available on Kaggle:    - Kaggle Housing Prices </p>"},{"location":"project01/#python-library-for-machine-learning-scikit-learn","title":"Python Library for Machine Learning: scikit-learn","text":"<p>We use scikit-learn, built on NumPy, SciPy, and matplotlib    - Read more at https://scikit-learn.org/    - Scikit-learn supports classification, regression, and clustering.    - This project applies regression.</p>"},{"location":"project01/#professional-python-setup-and-workflow","title":"Professional Python Setup and Workflow","text":"<p>We follow professional Python practices.  Full instructions are available at https://github.com/denisecase/pro-analytics-02/. </p> <p>Important: VS Code + Pylance may fail to recognize installed packages in Jupyter notebooks. See the above guides for troubleshooting and solutions.  </p>"},{"location":"project01/#project-outline","title":"Project Outline","text":"<p>Machine learning projects follow a structured approach. We will use this approach throughout the course. </p> <p>Start your notebook professionally with: - a single top-level title - your name (or alias) - the date - a brief introduction that describes the problem and the dataset. - Import the external Python libraries used (e.g., pandas, numpy, matplotlib, seaborn, sklearn, etc.)</p> <p>Present your work in clearly numbered second-level and third-level headings</p>"},{"location":"project01/#section-1-load-and-explore-the-data","title":"Section 1. Load and Explore the Data","text":"<ul> <li>1.1 Load the dataset and display the first 10 rows.</li> <li>1.2 Check for missing values and display summary statistics.</li> </ul> <p>Analysis: What do you notice about the dataset? Are there any data issues?</p>"},{"location":"project01/#section-2-visualize-feature-distributions","title":"Section 2. Visualize Feature Distributions","text":"<ul> <li>2.1 Create histograms, boxplots, and scatterplots.</li> <li>2.2 Identify patterns or anomalies in feature distributions.</li> </ul> <p>Analysis: What patterns or anomalies do you see? Do any features stand out?</p>"},{"location":"project01/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":"<ul> <li>3.1 Choose two input features for predicting the target.</li> <li>3.2 Justify your selection with reasoning.</li> </ul> <p>Analysis: Why did you choose these features? How might they impact predictions?</p>"},{"location":"project01/#section-4-train-a-linear-regression-model","title":"Section 4. Train a Linear Regression Model","text":"<ul> <li>4.1 Define X (features) and y (target).</li> <li>4.2 Train a Linear Regression model using Scikit-Learn.</li> <li>4.3 Report R^2, MAE, RMSE.</li> </ul> <p>Analysis: How well did the model perform? Any surprises in the results?</p> <p>See EXAMPLE_ANALYSIS for more.</p>"},{"location":"project01/#readmemd-required","title":"README.md (Required)","text":"<p>Include a professional README.md. Include: - a personalized title - an introduction to your project - a clickable link to your notebook file. - Instructions on how to set up your virtual environment and run your notebook locally.</p> <p>If starting with an assignment README, remove the parts you do not need to present your project.</p>"},{"location":"project01/EXAMPLE_ANALYSIS/","title":"Project 1 - Analysis","text":"<p>Your content here...</p>"},{"location":"project02/","title":"Project 02","text":""},{"location":"project02/#titanic-dataset-analysis","title":"Titanic Dataset Analysis","text":"<p>Author: AARON  Date: October, 29, 2025  Objective: Untilize the first four steps in the model creation process with the Titanic dataset.</p>"},{"location":"project02/#introduction","title":"Introduction","text":"<ul> <li>This project uses the Titanic dataset to Explore and Clean data, choose a feature to predict, and split the dataset into train and test subsets.</li> </ul>"},{"location":"project02/#note","title":"Note:","text":"<ul> <li>I think it is appropriate to have data examples and graphs in the notebook rather than copying into the README.  - Here I include the oultine of the project and reflections for summaries.</li> </ul>"},{"location":"project02/#include-imports","title":"Include Imports","text":""},{"location":"project02/#section-1-load-and-explore-the-data","title":"Section 1. Load and Explore the Data","text":""},{"location":"project02/#11-load-the-dataset-and-display-basic-information","title":"1.1 Load the dataset and display basic information","text":""},{"location":"project02/#12-check-for-missing-values-and-display-summary-statistics","title":"1.2 Check for missing values and display summary statistics","text":""},{"location":"project02/#section-2-data-exploration-and-preparation","title":"Section 2. Data Exploration and Preparation","text":""},{"location":"project02/#21-explore-data-patterns-and-distributions","title":"2.1 Explore Data Patterns and Distributions","text":""},{"location":"project02/#reflection-21","title":"Reflection 2.1:","text":"<ul> <li>What patterns or anomalies do you notice?  <ol> <li>There were a lot of very young children aboard.  </li> <li>There must have been some reason third class had such a high mortality rate.</li> </ol> </li> <li>Do any features stand out as potential predictors? It looks like higher fares were paid by women.</li> <li>Are there any visible class imbalances? Younger people appear to be in third class.</li> </ul>"},{"location":"project02/#22-handle-missing-values-and-clean-data","title":"2.2 Handle Missing Values and Clean Data","text":""},{"location":"project02/#23-feature-engineering","title":"2.3 Feature Engineering","text":""},{"location":"project02/#reflection-23","title":"Reflection 2.3","text":"<ul> <li>Why might family size be a useful feature for predicting survival?  </li> <li>This may help if a families were together.  I suspect there were seperations though.</li> <li>Why convert categorical data to numeric? It helps for mathematical cacluations for models and one-hop encoding.</li> </ul>"},{"location":"project02/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":""},{"location":"project02/#31-choose-features-and-target","title":"3.1 Choose features and target","text":""},{"location":"project02/#32-define-x-and-y","title":"3.2 Define X and y","text":"<ul> <li>Input features: age, fare, pclass, sex, family_size</li> <li>Target: survived</li> </ul>"},{"location":"project02/#reflection-3","title":"Reflection 3:","text":"<ul> <li>Why are these features selected?  These were determined in anaylsis above to have an impact.</li> <li>Are there any features that are likely to be highly predictive of survival?  I think class and sex will be the most predictive.</li> </ul>"},{"location":"project02/#section-4-splitting","title":"Section 4. Splitting","text":"<ul> <li>Split the data into training and test sets using train_test_split first and StratifiedShuffleSplit second. Compare.</li> </ul>"},{"location":"project02/#41-basic-traintest-split","title":"4.1 Basic Train/Test split","text":"<p>Original Class Distribution:</p> <ul> <li>Class 3    0.551066</li> <li>Class 1    0.242424</li> <li>Class 2    0.206510</li> </ul> <p>Train Set Class Distribution:</p> <ul> <li>Class 3    0.557584</li> <li>Class 1    0.233146</li> <li>Class 2    0.209270</li> </ul> <p>Test Set Class Distribution:</p> <ul> <li>Class 3    0.525140</li> <li>Class 1    0.279330</li> <li>Class 2    0.195531</li> </ul>"},{"location":"project02/#42-stratified-traintest-split","title":"4.2 Stratified Train/Test split","text":"<p>Original Class Distribution:</p> <ul> <li>Class 3    0.551066</li> <li>Class 1    0.242424</li> <li>Class 2    0.206510</li> </ul> <p>Train Set Class Distribution:</p> <ul> <li>Class 3    0.561798</li> <li>Class 1    0.227528</li> <li>Class 2    0.210674</li> </ul> <p>Test Set Class Distribution:</p> <ul> <li>Class 3    0.508380</li> <li>Class 1    0.301676</li> <li>Class 2    0.189944</li> </ul>"},{"location":"project02/#43-compare-results","title":"4.3 Compare Results","text":""},{"location":"project02/#44-reflection-4","title":"4.4 Reflection 4","text":"<ul> <li>Why might stratification improve model performance?  I don't think stratification would improve model performance.  It looks to have a distribution less like the original class distribution than the basic class distribution. </li> <li>How close are the training and test distributions to the original dataset?  The stratisfied results look farther away from the original distribution for both the training and test set disbributions as compared to the basic one.</li> <li>Which split method produced better class balance?  The basic Train method looks to have a better class distribution to the original class distribution.</li> </ul>"},{"location":"project03/","title":"Project 3: Titanic Dataset Analysis","text":"<p>Author: AARON  Date: November, 4, 2025  Objective: Setup three model types, Decision Tree, Support Vector Machine, and Neural Network and use these to predict survival rate on the Titanic dataset.</p>"},{"location":"project03/#introduction","title":"Introduction","text":"<p>This project uses the Titanic dataset to Explore and Clean data, choose a feature to predict, and split the dataset into train and test subsets. Setup three model types, Decision Tree, Support Vector Machine, and Neural Network and use these to predict survival rate on the Titanic dataset.  I use three differet test cases with specific features of 'alone', 'age', and 'age' + 'family size'. Compare the output of these models and test cases against the others and see which, if any, predict survival rate better.</p>"},{"location":"project03/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":""},{"location":"project03/#31-choose-features-and-target","title":"3.1 Choose features and target","text":"<p>Select two or more input features (numerical for regression, numerical and/or categorical for classification) Use survived as the target.  We will do three input cases like the example. </p> <p>Case 1:  input features: alone target: survived</p> <p>Case 2: input features - age target: survived</p> <p>Case 3: input features -  age and family_size  target: survived</p>"},{"location":"project03/#reflection-3","title":"Reflection 3:","text":"<p>Why are these features selected?  These were selected based on reviewing the overview of the data. Are there any features that are likely to be highly predictive of survival?  I think age will be the most predictive.</p>"},{"location":"project03/#43-predict-and-evaluate-model-performance-decisiont-tree","title":"4.3 Predict and Evaluate Model Performance (Decisiont Tree)","text":"Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree Case 1 alone 63% 64% 63% 63% - Case 2 age 61% 58% 61% 55% - Case 3 age + family_size 59% 57% 59% 58% -"},{"location":"project03/#reflection-4","title":"Reflection 4:","text":"<p>How well did the different cases perform?  I for sure could see a disitction with the feaure 'alone' showing the best results. Are there any surprising results?  I was very suprised to see the 'alone' feature being a better predictor of survival than the age based features. Which inputs worked better? The 'alone' input worked the best.</p>"},{"location":"project03/#section-5-compare-alternative-models-svc-nn","title":"Section 5. Compare Alternative Models (SVC, NN)","text":"Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree Case 1 alone 63% 64% 63% 63% - Case 2 age 61% 58% 61% 55% - Case 3 age + family_size 59% 57% 59% 58% - ---------------------- -------- ------------------- ---------- ----------- -------- ----------- ------- SVM (RBF Kernel) Case 1 alone 63% 64% 63% 63% - Case 2 age 63% 66% 63% 52% - Case 3 age + family_size 63% 66% 63% 52% - ---------------------- -------- ------------------- ---------- ----------- -------- ----------- ------- Neural Network (MLP) Case 1 alone --- ---- --- -- - Case 2 age --- ---- --- -- - Case 3 age + family_size 66% 65% 66% 65% -"},{"location":"project03/#reflection-5","title":"Reflection 5:","text":"<p>How well did each of these new models/cases perform?  'The 'alone' case again performed the best in the SVM model type.  But the distinction between the three SVM cases narrowed as compared to the Decision Tree.  The Neural Network outperformed every other model. Are there any surprising results or insights?  I am surprised case 3 was so much better than the same case in the Decision Tree. Why might one model outperform the others?  I think the Neural Network performed better since it had two variables to work with and had feedback from both into each node.</p>"},{"location":"project03/#section-6-final-thoughts-insights","title":"Section 6. Final Thoughts &amp; Insights","text":"<p>At first I was a little discouraged at the accuracy of the models and other parameters.  It seemed like just knowing if a passenger was alone or not told most of the tale.  But when I worked with the Neural Network I saw noticeable improvement in the results.  I was glad to see that.  These differences can be seen in the table below. I look forward to using Neural Networks in the future for classification models.</p>"},{"location":"project04/","title":"Project 4 - Predicting a Continuous Target with Regression (Titanic)","text":"<p>Author: AARON  Date: November 14, 2025  Objective: Gain an understanding of four types of regression models and their characteristics.</p>"},{"location":"project04/#introduction","title":"Introduction","text":"<ul> <li>Using the Titanic dataset four use cases with linear regression will be explored.  The fourth case will have an engineered feature.  The best performing case will be used to see how four different regression models predict the values of fare price from the Titanic dataset.  This exercise should give some insights into how these four model types behave.</li> </ul>"},{"location":"project04/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":""},{"location":"project04/#31-choose-features-and-target","title":"3.1 Choose features and target","text":"<p>Case 1:  input features: 'Age' target: fare</p> <p>Case 2: input features - 'Family Size' target: fare</p> <p>Case 3: input features -  'Age' and 'Family Size' target: fare</p> <p>Case 4: input features -  'Class Survive' and 'Family Size' target: fare</p>"},{"location":"project04/#reflection-2-and-3","title":"Reflection 2 and 3:","text":"<ul> <li>Why might these features affect a passenger\u2019s fare:   I'm not sure age would help determine fare much except for younger children.  Family size would have some bearing as they could buy at a group rate.  </li> <li>List all available features:  survived, pclass, sex, age, sibsp, parch, embarked, class, who, adult_male, deck, embark_town, alive, alone </li> <li>Which other features could improve predictions and why:  I think class is the main determining factor on fare. </li> <li>How many variables are in your Case 4:  Three.  I combined class and survived to produce a number between -3 and 0.  I also have family size.</li> <li>Which variable(s) did you choose for Case 4 and why do you feel those could make good inputs:  I don't know for sure, but fare may have had an impact on who survived.  I am combining that with class to get two variable into one.  It also weights the equation nicely.  I'm real excited to see how this predicts the result.</li> </ul>"},{"location":"project04/#reflection-4","title":"Reflection 4:","text":"<p>Compare the train vs test results for each.</p> Model Type Case Features Used Training R\u00b2 Test R\u00b2 RMSE MAE Notes Decision Tree Case 1 Age 0.0099 0.0034 1441.84 25.28 - Case 2 Family Size 0.0499 0.0222 1414.62 25.02 - Case 3 Age, Family Size 0.0734 0.0497 1374.76 24.28 - Case 4 Family Size, Survived, Class 0.3293 0.4128 849.46 19.76 - <ul> <li>Did Case 1 overfit or underfit? Explain:  Case 1 is an underfit.  R2 are very low.</li> <li>Did Case 2 overfit or underfit? Explain:  Case 2 is an underfit.  The is a small improvement, but R2 are very low.</li> <li>Did Case 3 overfit or underfit? Explain:  Case 3 is an underfit.  The is a small improvement, R2 are very low.</li> <li>Did Case 4 overfit or underfit? Explain:  Case 4 is not overfit or underfit.  It's odd that the test set did better, but that does not mean it's overfit.</li> </ul>"},{"location":"project04/#adding-age","title":"Adding Age","text":"<ul> <li>Did adding age improve the model: The model improved slightly from Case 2 to Case 3.</li> <li>Propose a possible explanation (consider how age might affect ticket price, and whether the data supports that):  I think age coupled with family size would improve where an entry was a child who was part of a family.  The fare price for the family would account for that.</li> </ul>"},{"location":"project04/#worst","title":"Worst","text":"<ul> <li>Which case performed the worst: Case 1 - Age</li> <li>How do you know: The R2 value was almost 0.</li> <li>Do you think adding more training data would improve it (and why/why not):  I don't think adding more training data would help.  Age does not make a meaningful impact.</li> </ul>"},{"location":"project04/#best","title":"Best","text":"<ul> <li>Which case performed the best: Case 4 - Family Size, Class + Survived</li> <li>How do you know: The R2 was 10 times better. and the RMSE and MAE we noticeable lower.</li> <li>Do you think adding more training data would improve it (and why/why not):  I'm not sure.  In my case the test set performed better than the training set.  So it makes me think I had a good split or I maxed out any training data.</li> </ul>"},{"location":"project04/#54-reflections-in-a-markdown-cell","title":"5.4 Reflections (in a Markdown cell):","text":"<ul> <li>What patterns does the cubic model seem to capture:  For smaller family sizes it predicted lower fare values.</li> <li>Where does it perform well or poorly:  It does a little better than a straight linear line does when the family size is bigger.</li> <li>Did the polynomial fit outperform linear regression: Yes the R2 value 0.088 for poly but 0.022 for Linear.</li> <li>Where (on the graph or among which kinds of data points) does it fit best:  The prediction is best for two family members.  Followed by alone and then three family members.</li> </ul> Model Type Case Features Used Test R\u00b2 RMSE MAE Notes Linear Case 2 Family Size 0.022 1414.62 25.02 - Poly Cubic Case 2 Family Size 0.088 1319.24 24.07 -"},{"location":"project04/#cubic-vs-5th-order-polynomial-comparison","title":"Cubic vs 5th Order Polynomial Comparison","text":"<ul> <li>The 5th order Polynomial underperforms the cubic Polynominal model because it shows as almost a straight line for the first six family number values.  It is almost linear for those first six feature points.  The last two feature points it dropped aggressively which did not really help the prediction.  </li> <li>Although the Cubic Polynomial was not great, it did better than linear and the 5th order.  The main reason is seen by the prediction line bending after feature point three instead of a steady slope up which does not correspond to the actual outcomes.</li> </ul> Model Type Case Features Used Test R\u00b2 RMSE MAE Notes Linear Case 4 Family Size, Class + Survived 0.413 849.46 19.77 - Ridge Case Case 4 Family Size, Class + Survived 0.413 849.11 19.75 - ElasticNet Case 4 Family Size, Class + Survived 0.429 825.78 18.18 - Polynomial Cubic Case 4 Family Size, Class + Survived 0.469 768.20 15.49 - Polynomial Cubic Case 2 Family Size 0.088 1319.24 24.07 - Poly 5th Order Case 2 Family Size 0.065 1352.24 24.34 -"},{"location":"project04/#section-6-final-thoughts-insights","title":"Section 6. Final Thoughts &amp; Insights","text":""},{"location":"project04/#61-summarize-findings","title":"6.1 Summarize Findings","text":"<ul> <li> <p>What features were most useful?  The engineered feature class and survival had the largest positive effect on predicting fare.</p> </li> <li> <p>What regression model performed best?  The Polynomial Cubic Model iterated on Case 4 had the best metrics. Yet, I don't think it is the best model.  I had a hard time visualizing these models.  I added a \"residuals\" graphic to each of Case 4 models.  </p> </li> <li>The goal is to have it evenly distributed around 0 all the way through the range with as little amplitude as possible.</li> <li> <p>The first three models had that characteristic for 2/3 of the range and then started having higher amplitudes at the larger fare values. However, the Polynomial Cubic Model had only three areas with very high amplitude.  This leads me to believe that it has overfitted the training data and that Ridge or ElasticNet should be used. </p> </li> <li> <p>How did model complexity or regularization affect results?  I think regularization had some positive effect, but it was pretty minor.  I was a little disappointed Ridge and ElasticNet did not have a higher impact.  I suspect the input features would need to be adjusted for more of a step function change.</p> </li> <li> <p>Further Discussion:  I chose to combine class and survived as a engineered feature.  After doing some more reading that may have a causality type effect.  I'll need to keep an eye out for this in the future, but I feel like the use of both here enhanced the prediction without causing a causality type effect.</p> </li> </ul>"},{"location":"project04/#62-discuss-challenges","title":"6.2 Discuss Challenges","text":"<ul> <li> <p>Was fare hard to predict? Why?  Fare was hard to predict with the given inputs.  There was a very low R2 value for Age, Family Size, and even for Age and Family Size engineered parameter.  Class really is what helped the models perform up to 10 times better when coupled with Family Size. </p> </li> <li> <p>Did skew or outliers impact the models?  Outliers had a decided effect on the models.  A few points were so high with no real basis in the features used in the models.</p> </li> </ul>"},{"location":"project04/#how-to-run-the-jupyter-notebook","title":"How to Run the Jupyter Notebook","text":"<p>This Jupyter Notebook utilizes some packages in the uv environment and will install some dependencies.  The following set of commands would help you complete these two objectives. </p> <pre><code>uv venv\nuv python pin 3.12\nuv sync --extra dev --extra docs --upgrade\nuv run python --version\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <p>macOS / Linux / WSL:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>There are imports at the top of the Jupyter Notebook that should also be run to utilize the installed software and run the code in the rest of the Jupyter Notebook.</p>"},{"location":"project05/","title":"Project 5 Research on Ensemble Models (Red Wine)","text":"<p>Author: AARON  Date: November 19, 2025  Objective: Gain an understanding of ensemble model setup and performance refinement.</p>"},{"location":"project05/#introduction","title":"Introduction","text":"<p>I have three objectives in this project: - Gain an understanding of ensemble model setup and performance refinement. - Compare work with other peers' research. - Give a summary of findings.</p>"},{"location":"project05/#section-1-import-and-inspect-the-data","title":"Section 1. Import and Inspect the Data","text":""},{"location":"project05/#section-2-data-preparation","title":"Section 2. Data Preparation","text":""},{"location":"project05/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":""},{"location":"project05/#section-4-split-the-data-into-train-and-test","title":"Section 4. Split the Data into Train and Test","text":""},{"location":"project05/#section-5-evaluate-model-performance-choose-2","title":"Section 5.  Evaluate Model Performance (Choose 2)","text":""},{"location":"project05/#51a-gradient-boosting-100","title":"5.1a Gradient Boosting (100)","text":"<p>Gradient Boosting (100, 0.1) Results Confusion Matrix (Test): [[  0  13   0]  [  3 247  14]  [  0  16  27]] Train Accuracy: 0.9601, Test Accuracy: 0.8562 Train F1 Score: 0.9584, Test F1 Score: 0.8411</p>"},{"location":"project05/#51b-gradient-boosting-175","title":"5.1b Gradient Boosting (175)","text":"<p>Gradient Boosting (175, 0.03) Results Confusion Matrix (Test): [[  1  12   0]  [  2 250  12]  [  0  16  27]] Train Accuracy: 0.9281, Test Accuracy: 0.8688 Train F1 Score: 0.9222, Test F1 Score: 0.8546</p>"},{"location":"project05/#51c-reflection-on-gradient-boosting","title":"5.1c Reflection on Gradient Boosting","text":"<ul> <li>I think a well defined improvement was made by increasing the n_estimators to 175 and decreasing the learning_rate to 0.03.  I see a Test Accuracy and Test F1 Score improvement of &gt; 1%, an accurate prediction for one \"low\" classification, and a 3% drop in the Train Accuracy and Train F1 Score.  </li> <li>I gain accuracy and reduce the difference between the Train and Test metrics from 10% to 6%.  This in turn leads to less overfitting.  </li> <li>I feel this is a great addition to this model and something to keep in mind.  Namely, adjust the parameters of the model to get a good balance.</li> </ul>"},{"location":"project05/#52a-voting-rf-100-lr-1k-knn","title":"5.2a Voting (RF 100 + LR 1K + KNN)","text":"<p>Voting (RF 100 + LR 1K + KNN) Results Confusion Matrix (Test): [[  0  13   0]  [  0 258   6]  [  0  27  16]] Train Accuracy: 0.9179, Test Accuracy: 0.8562 Train F1 Score: 0.9010, Test F1 Score: 0.8236</p>"},{"location":"project05/#52b-voting-rf-100-lr-1k-knn","title":"5.2b Voting (RF 100 + LR 1K + KNN)","text":"<p>Voting (RF 300 + LR 2K + KNN) Results Confusion Matrix (Test): [[  0  13   0]  [  1 258   5]  [  1  23  19]] Train Accuracy: 0.9124, Test Accuracy: 0.8656 Train F1 Score: 0.8977, Test F1 Score: 0.8391</p>"},{"location":"project05/#52c-reflection-on-voting","title":"5.2c Reflection on Voting","text":"<ul> <li>Here I found that fine tuning the parameters increased accuracy and reduced overfitting just like I saw in Gradient Boosting refinement.  It was only 1 percent, but it improved the counts in the \"High\" correct category by 3 so about 18%.  This is important because in a lot of cases we are trying to find what makes the wine the best.  We want to identify those high performing combinations correctly.  </li> <li>If we were just using one model instead of three I think overfitting would happen by increasing the parameters the way I did.  But according to the data, the Training Accuracy and Training F1 values went down.  This is good news for the overfitting problem.</li> <li>I also tried changing from \"soft\" voting to \"hard\" voting.  This also proved helpful.  Just going with the most vote getters instead of the probability of the three was better.</li> </ul>"},{"location":"project05/#section-6-compare-results","title":"Section 6. Compare Results","text":""},{"location":"project05/#61-make-the-comparison-table","title":"6.1 Make the comparison table.","text":""},{"location":"project05/#62-comparison-table-for-the-data-i-studied","title":"6.2 Comparison table for the data I studied.","text":"Model Type Train Accuracy Test Accuracy Train F1 Test F1 Test Acc_Diff Train-Test Acc_Diff Train-Test F1_Diff Gradient Boosting (175, 0.03) 0.928069 0.868750 0.922212 0.854639 0.012500 0.059319 0.067573 Voting (RF 300 + LR 2K + KNN) 0.912432 0.865625 0.897654 0.839116 0.009375 0.046807 0.058538 Gradient Boosting (100, 0.1) 0.960125 0.856250 0.958410 0.841106 0.000000 0.103875 0.117304 Voting (RF 100 + LR 1K + KNN) 0.917905 0.856250 0.901043 0.823627 0.000000 0.061655 0.077416"},{"location":"project05/#63-comparison-table-for-the-data-i-studied-with-additional-data-from-dan-miller-and-megan-chastain-research","title":"6.3 Comparison table for the data I studied with additional data from Dan Miller and Megan Chastain research.","text":"Model Type Train Accuracy Test Accuracy Train F1 Test F1 Train-Test Acc_Diff Train-Test F1_Diff Random Forest 1.0 0.888 1.0 0.866 0.112 0.134 Bagging 1.0 0.884 1.0 0.865 0.116 0.135 Gradient Boosting (175, 0.03) 0.928069 0.868750 0.922212 0.854639 0.059319 0.067573 Voting (RF 300 + LR 2K + KNN) 0.912432 0.865625 0.897654 0.839116 0.046807 0.058538 Gradient Boosting (100, 0.1) 0.960125 0.856250 0.958410 0.841106 0.103875 0.117304 Voting (RF 100 + LR 1K + KNN) 0.917905 0.856250 0.901043 0.823627 0.061655 0.077416 Voting (DT + SVM + NN) 0.916341 0.85625 0.897973 0.831882 0.060091 0.066091 AdaBoost 0.840 0.856 0.816 0.833 -0.017 -0.017 MLP Classifier 0.851446 0.84375 0.814145 0.807318 0.007696 0.006827"},{"location":"project05/#section-7-conclusions-and-insights","title":"Section 7. Conclusions and Insights","text":"<ul> <li>By far the biggest takeaway for me from this project is to make overfitting reduction a priority.  Second, work with parameters of the models to reduce overfitting and increase accuracy.</li> <li>I studied Gradient Boosting and Voting (RF + RL + KNN).  I found an appreciable difference in both accuracy and overfitting when I fine tuned the parameters.</li> <li>I actually would use Gradient Boosting (175, 0.03) and Voting (RF 300 + LR 2K + KNN) over any of the models I reviewed for the following reasons.  </li> <li>The Random Forest and Bagging Test and F1 Accuracy were better, but I think they were overfitting.  The difference for them both between the Train and Test Accuracy was 11.6%.  Also their Train Accuracy came in at 100%.</li> <li>In contrast, the Gradient Boosting (175, 0.03) model had a Train and Test Difference of just 6%. And Voting (RF 300 + LR 2K + KNN) came in at 5%.</li> <li>Both of these refined models had higher Test Accuracies than the other models reviewed.</li> </ul>"},{"location":"project05/#section-8-next-steps","title":"Section 8.  Next Steps","text":"<ul> <li>I feel the best next step would be to review the model parameters on each of the other models.  I did not go through everyone's results to see if others did this.  I suspect Random Forest and Bagging models could have parameters changed to reduce overfitting.  I also think some of the lower performing models could be tuned to get a better test accuracy without causing overfitting.  The two models I reviewed were just par with most of the others until I dove into tuning them.</li> <li>After that, I like the voting system.  I would just add a few more in and keep an odd number.  Then I would run voting with \"soft\" and \"hard\" voting to see what did best.  I do think voting helps reduce overfitting.</li> </ul>"},{"location":"project05/#acknowledgments","title":"Acknowledgments:","text":"<p>I referenced Dan Miller and Megan Chastain research in these results.</p> <p>Dan Miller's referenced model metrics can be found in notebook: https://github.com/DMill31/applied-ml-miller/blob/main/notebooks/project05/ensemble-miller.ipynb</p> <p>Megan Chastain referenced model metrics can be found in notebook: https://github.com/Megan-Chastain1/applied-ml-Chastain/blob/main/docs/project05/%20ensemble-Chastain.ipynb</p>"},{"location":"project05/#how-to-run-the-jupyter-notebook","title":"How to Run the Jupyter Notebook","text":"<p>This Jupyter Notebook utilizes some packages in the uv environment and will install some dependencies.  The following set of commands would help you complete these two objectives. </p> <pre><code>uv venv\nuv python pin 3.12\nuv sync --extra dev --extra docs --upgrade\nuv run python --version\n</code></pre> <p>Windows (PowerShell):</p> <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <p>macOS / Linux / WSL:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>There are imports at the top of the Jupyter Notebook that should also be run to utilize the installed software and run the code in the rest of the Jupyter Notebook.</p>"}]}