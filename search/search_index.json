{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Applied ML Projects","text":"<p>Author: Aaron</p> <p>I have updated Project 3 which can be accessed by the link below.</p> <p>Projects index:</p> <ul> <li>Midterm Project: Banknote Authentication Analysis</li> <li>Project 01: Title Here</li> <li>Project 02: Exploring the Titanic Dataset</li> <li>Project 03: Three Model Types with the Titanic Dataset</li> <li>Project 04: Title Here</li> </ul>"},{"location":"midterm/","title":"Machine Learning Midterm Project: Banknote Authentication Analysis","text":"<p>Author: AARON  Date: November, 8, 2025  Objective: Setup two model types, Neural Network and Decision Tree, and use these to predict authenticity of bank notes from the UCI Banknote Authentication Dataset.</p>"},{"location":"midterm/#introduction","title":"Introduction","text":"<ul> <li>1st, This project uses the UCI Banknote Authentication Dataset to Explore and Clean data, choose features used to predict authenticity of bank notes, and split the dataset into train and test subsets.</li> <li>2nd, Two model types, Neural Network and Decision Tree, will be used to predict authenticity of bank notes from the UCI Banknote Authentication Dataset.</li> <li>3rd, Reflections are include from the sections of the notebook.  These reflections are summaries of why choices were made and key points from the data analysis.  </li> </ul>"},{"location":"midterm/#reflection-1","title":"Reflection 1:","text":"<ul> <li>1) How many data instances are there? 1372</li> <li>2) How many features are there? 4</li> <li>3) What are the names?  Variance;     Skewness;     Curtosis;      Entropy;            </li> <li>4) Are there any missing values? No</li> <li>5) Are there any non-numeric features? Yes.  All four features are numeric.</li> <li>6) Are the data instances sorted on any of the attributes?  There is no a sort on any attribute.</li> <li>7) What two different features have the highest correlation? Skewness and Entropy against Variance</li> <li>8) Are there any categorical features that might be useful for prediction?  Perhaps entropy and curtosis would be a good place to start.</li> </ul>"},{"location":"midterm/#reflection-2-and-3","title":"Reflection 2 and 3:","text":"<ul> <li>Why are these features selected?  Curtosis, Variance, and Skewness were selected based on reviewing the overview of the data in the scatter matrix.  I saw that when skewness and variance are high in value there is a tendency for genuine notes.  I also saw when there is a lower curtosis value the tendency it towards genuine notes.  So I want to focus on these.  </li> <li>I chose for case 3 a feature engineered variable where I add the values of skewness and variance to heighten the positive values weight of the parameters.  I think that will help the model out even more.</li> </ul> <p>Decision Tree Data</p> Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree Case 1 Curtosis, Variance 91.6% 91.7% 91.6% 91.6% - Case 2 Skewness, Variance 93.1% 93.2% 93.1% 93.1% - Case 3 Curtosis, skew_var 98.6% 98.6% 98.6% 98.5% -"},{"location":"midterm/#reflection-4-decision-tree-models","title":"Reflection 4 (Decision Tree Models):","text":"<ul> <li>How well did the different cases perform?  I was happy to see all of the parameters at or above 91%.</li> <li>Are there any surprising results?  I was surprised that all the training values were 100%.  I'm not sure why that is. I was surprised to see that the engineered feature had such a big impact.</li> <li>Which inputs worked better? I think Skewness along with Variance is a key parameter since Case 2 outperformed.  I am excited to see how these cases will turn out in a Neural Network model.</li> </ul>"},{"location":"midterm/#reflection-5-neural-network-models","title":"Reflection 5 (Neural Network Models):","text":"<ul> <li>How well did each of these neural network cases perform?  The Neural network cases with Skewness outperformed the decision tree.  But Case 1 for the NN was significantly worse at predicting that all other instances in NN or Decision Tree</li> <li>Are there any surprising results or insights?  I am surprised Skewness seemed to have such a strong effect on the metrics.  The only difference between Case 1 and Case 3 was skewness and it had a 7% difference in the results.</li> <li>Why might one model outperform the others?  Looking at the Neural Network graphs it looks like a surface with points plotted over areas that the model predicts.  That makes sense to me that the NN can make good predictions over asking questions in the decision tree.</li> </ul> <p>Neural Network and Decision Tree Data Metric</p> Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree Case 1 Curtosis, Variance 91.6% 91.7% 91.6% 91.6% - Case 2 Skewness, Variance 93.1% 93.2% 93.1% 93.1% - Case 3 Curtosis, skew_var 98.6% 98.6% 98.6% 98.5% - --------------- -------- -------------------- --------- ----------- -------- ----------- ------- Neural Network Case 1 Curtosis, Variance 86.2% 86.2% 86.2% 86.2% - Case 2 Skewness, Variance 94.6% 94.6% 94.6% 94.6% - Case 3 Curtosis, skew_var 99.6% 99.6% 99.6% 99.6% -"},{"location":"midterm/#section-6-final-thoughts-insights","title":"Section 6. Final Thoughts &amp; Insights","text":"<ul> <li>The accuracy and other metrics were higher for this dataset compared to the Titanic dataset.  It was nice to see a higher percentage for these metrics.  </li> <li>I'm not sure why the Neural Network for Case 1 shows such a low percentage compared to other NN Cases, but also compared to Case 1 in the Decision tree.  It seems there must be an error, but I have checked a few times and cannot locate why this happened.</li> <li>I got the idea to combine the Skewness and Variance from the Scatter Matrix.  I saw Genuine notes had higher values for both these parameters.  So I thought I could combine them into one variable and that worked out great.</li> <li>I have not tried three parameters in a neural network.  I liked the decisions surface for two inputs.  So I combined Skewness and Variance.</li> <li>At first I used the Case 3 engineered parameter as Skewness Times Variance.  It performed a little worse than CASE 2.  I thought I would try adding them instead, towards the end of the project, and the numbers returned tremendous.  There was only one misclassification in the CASE 3 Neural Network results.</li> <li> </li> <li> <p>I was happy with these results and glad inspection of the scatter plot let me to combine the correct features.</p> </li> </ul>"},{"location":"project01/","title":"Project 01","text":""},{"location":"project01/#overview","title":"Overview","text":"<p>Businesses and organizations often need to understand the relationships between different factors to make better decisions. For example, a company may want to predict the fuel efficiency of a car based on its weight and engine size or estimate home prices based on square footage and location. Regression analysis helps identify and quantify these relationships between numerical features, providing insights that can be used for forecasting and decision-making.</p> <p>This project demonstrates your ability to apply regression modeling techniques to a real-world dataset. You will: - Load and explore a dataset. - Choose and justify features for predicting a target variable. - Train a regression model and evaluate performance. - Compare multiple regression approaches. - Document your work in a structured Jupyter Notebook.</p>"},{"location":"project01/#dataset","title":"Dataset","text":"<p>Housing Prices Dataset (Predict home values based on features like square footage and location) - We use the built-in dataset from scikit-learn:    - <code>from sklearn.datasets import fetch_california_housing</code> - Additional dataset available on Kaggle:    - Kaggle Housing Prices </p>"},{"location":"project01/#python-library-for-machine-learning-scikit-learn","title":"Python Library for Machine Learning: scikit-learn","text":"<p>We use scikit-learn, built on NumPy, SciPy, and matplotlib    - Read more at https://scikit-learn.org/    - Scikit-learn supports classification, regression, and clustering.    - This project applies regression.</p>"},{"location":"project01/#professional-python-setup-and-workflow","title":"Professional Python Setup and Workflow","text":"<p>We follow professional Python practices.  Full instructions are available at https://github.com/denisecase/pro-analytics-02/. </p> <p>Important: VS Code + Pylance may fail to recognize installed packages in Jupyter notebooks. See the above guides for troubleshooting and solutions.  </p>"},{"location":"project01/#project-outline","title":"Project Outline","text":"<p>Machine learning projects follow a structured approach. We will use this approach throughout the course. </p> <p>Start your notebook professionally with: - a single top-level title - your name (or alias) - the date - a brief introduction that describes the problem and the dataset. - Import the external Python libraries used (e.g., pandas, numpy, matplotlib, seaborn, sklearn, etc.)</p> <p>Present your work in clearly numbered second-level and third-level headings</p>"},{"location":"project01/#section-1-load-and-explore-the-data","title":"Section 1. Load and Explore the Data","text":"<ul> <li>1.1 Load the dataset and display the first 10 rows.</li> <li>1.2 Check for missing values and display summary statistics.</li> </ul> <p>Analysis: What do you notice about the dataset? Are there any data issues?</p>"},{"location":"project01/#section-2-visualize-feature-distributions","title":"Section 2. Visualize Feature Distributions","text":"<ul> <li>2.1 Create histograms, boxplots, and scatterplots.</li> <li>2.2 Identify patterns or anomalies in feature distributions.</li> </ul> <p>Analysis: What patterns or anomalies do you see? Do any features stand out?</p>"},{"location":"project01/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":"<ul> <li>3.1 Choose two input features for predicting the target.</li> <li>3.2 Justify your selection with reasoning.</li> </ul> <p>Analysis: Why did you choose these features? How might they impact predictions?</p>"},{"location":"project01/#section-4-train-a-linear-regression-model","title":"Section 4. Train a Linear Regression Model","text":"<ul> <li>4.1 Define X (features) and y (target).</li> <li>4.2 Train a Linear Regression model using Scikit-Learn.</li> <li>4.3 Report R^2, MAE, RMSE.</li> </ul> <p>Analysis: How well did the model perform? Any surprises in the results?</p> <p>See EXAMPLE_ANALYSIS for more.</p>"},{"location":"project01/#readmemd-required","title":"README.md (Required)","text":"<p>Include a professional README.md. Include: - a personalized title - an introduction to your project - a clickable link to your notebook file. - Instructions on how to set up your virtual environment and run your notebook locally.</p> <p>If starting with an assignment README, remove the parts you do not need to present your project.</p>"},{"location":"project01/EXAMPLE_ANALYSIS/","title":"Project 1 - Analysis","text":"<p>Your content here...</p>"},{"location":"project02/","title":"Project 02","text":""},{"location":"project02/#titanic-dataset-analysis","title":"Titanic Dataset Analysis","text":"<p>Author: AARON  Date: October, 29, 2025  Objective: Untilize the first four steps in the model creation process with the Titanic dataset.</p>"},{"location":"project02/#introduction","title":"Introduction","text":"<ul> <li>This project uses the Titanic dataset to Explore and Clean data, choose a feature to predict, and split the dataset into train and test subsets.</li> </ul>"},{"location":"project02/#note","title":"Note:","text":"<ul> <li>I think it is appropriate to have data examples and graphs in the notebook rather than copying into the README.  - Here I include the oultine of the project and reflections for summaries.</li> </ul>"},{"location":"project02/#include-imports","title":"Include Imports","text":""},{"location":"project02/#section-1-load-and-explore-the-data","title":"Section 1. Load and Explore the Data","text":""},{"location":"project02/#11-load-the-dataset-and-display-basic-information","title":"1.1 Load the dataset and display basic information","text":""},{"location":"project02/#12-check-for-missing-values-and-display-summary-statistics","title":"1.2 Check for missing values and display summary statistics","text":""},{"location":"project02/#section-2-data-exploration-and-preparation","title":"Section 2. Data Exploration and Preparation","text":""},{"location":"project02/#21-explore-data-patterns-and-distributions","title":"2.1 Explore Data Patterns and Distributions","text":""},{"location":"project02/#reflection-21","title":"Reflection 2.1:","text":"<ul> <li>What patterns or anomalies do you notice?  <ol> <li>There were a lot of very young children aboard.  </li> <li>There must have been some reason third class had such a high mortality rate.</li> </ol> </li> <li>Do any features stand out as potential predictors? It looks like higher fares were paid by women.</li> <li>Are there any visible class imbalances? Younger people appear to be in third class.</li> </ul>"},{"location":"project02/#22-handle-missing-values-and-clean-data","title":"2.2 Handle Missing Values and Clean Data","text":""},{"location":"project02/#23-feature-engineering","title":"2.3 Feature Engineering","text":""},{"location":"project02/#reflection-23","title":"Reflection 2.3","text":"<ul> <li>Why might family size be a useful feature for predicting survival?  </li> <li>This may help if a families were together.  I suspect there were seperations though.</li> <li>Why convert categorical data to numeric? It helps for mathematical cacluations for models and one-hop encoding.</li> </ul>"},{"location":"project02/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":""},{"location":"project02/#31-choose-features-and-target","title":"3.1 Choose features and target","text":""},{"location":"project02/#32-define-x-and-y","title":"3.2 Define X and y","text":"<ul> <li>Input features: age, fare, pclass, sex, family_size</li> <li>Target: survived</li> </ul>"},{"location":"project02/#reflection-3","title":"Reflection 3:","text":"<ul> <li>Why are these features selected?  These were determined in anaylsis above to have an impact.</li> <li>Are there any features that are likely to be highly predictive of survival?  I think class and sex will be the most predictive.</li> </ul>"},{"location":"project02/#section-4-splitting","title":"Section 4. Splitting","text":"<ul> <li>Split the data into training and test sets using train_test_split first and StratifiedShuffleSplit second. Compare.</li> </ul>"},{"location":"project02/#41-basic-traintest-split","title":"4.1 Basic Train/Test split","text":"<p>Original Class Distribution:</p> <ul> <li>Class 3    0.551066</li> <li>Class 1    0.242424</li> <li>Class 2    0.206510</li> </ul> <p>Train Set Class Distribution:</p> <ul> <li>Class 3    0.557584</li> <li>Class 1    0.233146</li> <li>Class 2    0.209270</li> </ul> <p>Test Set Class Distribution:</p> <ul> <li>Class 3    0.525140</li> <li>Class 1    0.279330</li> <li>Class 2    0.195531</li> </ul>"},{"location":"project02/#42-stratified-traintest-split","title":"4.2 Stratified Train/Test split","text":"<p>Original Class Distribution:</p> <ul> <li>Class 3    0.551066</li> <li>Class 1    0.242424</li> <li>Class 2    0.206510</li> </ul> <p>Train Set Class Distribution:</p> <ul> <li>Class 3    0.561798</li> <li>Class 1    0.227528</li> <li>Class 2    0.210674</li> </ul> <p>Test Set Class Distribution:</p> <ul> <li>Class 3    0.508380</li> <li>Class 1    0.301676</li> <li>Class 2    0.189944</li> </ul>"},{"location":"project02/#43-compare-results","title":"4.3 Compare Results","text":""},{"location":"project02/#44-reflection-4","title":"4.4 Reflection 4","text":"<ul> <li>Why might stratification improve model performance?  I don't think stratification would improve model performance.  It looks to have a distribution less like the original class distribution than the basic class distribution. </li> <li>How close are the training and test distributions to the original dataset?  The stratisfied results look farther away from the original distribution for both the training and test set disbributions as compared to the basic one.</li> <li>Which split method produced better class balance?  The basic Train method looks to have a better class distribution to the original class distribution.</li> </ul>"},{"location":"project03/","title":"Project 3: Titanic Dataset Analysis","text":"<p>Author: AARON  Date: November, 4, 2025  Objective: Setup three model types, Decision Tree, Support Vector Machine, and Neural Network and use these to predict survival rate on the Titanic dataset.</p>"},{"location":"project03/#introduction","title":"Introduction","text":"<p>This project uses the Titanic dataset to Explore and Clean data, choose a feature to predict, and split the dataset into train and test subsets. Setup three model types, Decision Tree, Support Vector Machine, and Neural Network and use these to predict survival rate on the Titanic dataset.  I use three differet test cases with specific features of 'alone', 'age', and 'age' + 'family size'. Compare the output of these models and test cases against the others and see which, if any, predict survival rate better.</p>"},{"location":"project03/#section-3-feature-selection-and-justification","title":"Section 3. Feature Selection and Justification","text":""},{"location":"project03/#31-choose-features-and-target","title":"3.1 Choose features and target","text":"<p>Select two or more input features (numerical for regression, numerical and/or categorical for classification) Use survived as the target.  We will do three input cases like the example. </p> <p>Case 1:  input features: alone target: survived</p> <p>Case 2: input features - age target: survived</p> <p>Case 3: input features -  age and family_size  target: survived</p>"},{"location":"project03/#reflection-3","title":"Reflection 3:","text":"<p>Why are these features selected?  These were selected based on reviewing the overview of the data. Are there any features that are likely to be highly predictive of survival?  I think age will be the most predictive.</p>"},{"location":"project03/#43-predict-and-evaluate-model-performance-decisiont-tree","title":"4.3 Predict and Evaluate Model Performance (Decisiont Tree)","text":"Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree Case 1 alone 63% 64% 63% 63% - Case 2 age 61% 58% 61% 55% - Case 3 age + family_size 59% 57% 59% 58% -"},{"location":"project03/#reflection-4","title":"Reflection 4:","text":"<p>How well did the different cases perform?  I for sure could see a disitction with the feaure 'alone' showing the best results. Are there any surprising results?  I was very suprised to see the 'alone' feature being a better predictor of survival than the age based features. Which inputs worked better? The 'alone' input worked the best.</p>"},{"location":"project03/#section-5-compare-alternative-models-svc-nn","title":"Section 5. Compare Alternative Models (SVC, NN)","text":"Model Type Case Features Used Accuracy Precision Recall F1-Score Notes Decision Tree Case 1 alone 63% 64% 63% 63% - Case 2 age 61% 58% 61% 55% - Case 3 age + family_size 59% 57% 59% 58% - ---------------------- -------- ------------------- ---------- ----------- -------- ----------- ------- SVM (RBF Kernel) Case 1 alone 63% 64% 63% 63% - Case 2 age 63% 66% 63% 52% - Case 3 age + family_size 63% 66% 63% 52% - ---------------------- -------- ------------------- ---------- ----------- -------- ----------- ------- Neural Network (MLP) Case 1 alone --- ---- --- -- - Case 2 age --- ---- --- -- - Case 3 age + family_size 66% 65% 66% 65% -"},{"location":"project03/#reflection-5","title":"Reflection 5:","text":"<p>How well did each of these new models/cases perform?  'The 'alone' case again performed the best in the SVM model type.  But the distinction between the three SVM cases narrowed as compared to the Decision Tree.  The Neural Network outperformed every other model. Are there any surprising results or insights?  I am surprised case 3 was so much better than the same case in the Decision Tree. Why might one model outperform the others?  I think the Neural Network performed better since it had two variables to work with and had feedback from both into each node.</p>"},{"location":"project03/#section-6-final-thoughts-insights","title":"Section 6. Final Thoughts &amp; Insights","text":"<p>At first I was a little discouraged at the accuracy of the models and other parameters.  It seemed like just knowing if a passenger was alone or not told most of the tale.  But when I worked with the Neural Network I saw noticeable improvement in the results.  I was glad to see that.  These differences can be seen in the table below. I look forward to using Neural Networks in the future for classification models.</p>"},{"location":"project04/","title":"Project 04","text":""}]}